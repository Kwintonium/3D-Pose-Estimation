{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model from tfhub.dev\n",
    "# model = hub.load('https://tfhub.dev/google/movenet/multipose/lightning/1') # Stopped working for some reason\n",
    "model = tf.saved_model.load('model\\\\')\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NVIDIA GPU for speed (comment out if you do not have a NVIDIA GPU or if previously ran)\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw circles on live frame where key points are located above certain threshold\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    # Loop through every keypoint, draw circle on frame if confidence_threshold is high\n",
    "    for kp in keypoints: # kp = keypoint\n",
    "        kpy, kpx, kpc = kp\n",
    "        if kpc > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kpx), int(kpy)), 6, (0,255,0), -1) # (image, (x, y), size, color, fill circle)\n",
    "            # cv2.putText(frame, 'text', (int(kpx), int(kpy)), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=(0,255,0), thickness=1)\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices (from the google colab on tfhub)\n",
    "keypoint_dict = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Dictionary that maps connected keypoints lines with a color (from the google colab on tfhub)\n",
    "edges = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "# Draw connections between keypoints\n",
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "\n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = keypoints[p1]\n",
    "        y2, x2, c2 = keypoints[p2]\n",
    "    \n",
    "        # If each point has high enough confidence, draw points connecting the two\n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):\n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)\n",
    "\n",
    "# Loop through 6 people with highest confidence scores\n",
    "def loop_through_people(frame, outputs, edges, confidence_threshold):\n",
    "    for people in outputs:\n",
    "        draw_connections(frame, people, edges, confidence_threshold)\n",
    "        draw_keypoints(frame, people, confidence_threshold)\n",
    "\n",
    "# Get outputs from the Movenet Model and store in outputs\n",
    "def get_outputs(frame):\n",
    "    # Reframe image to multiple of 32 and dtype to int32 per model definition\n",
    "    img = frame.copy()\n",
    "    img = tf.image.resize_with_pad(tf.expand_dims(img, axis=0), 192, 256) # Scale according to webcam\n",
    "    img = tf.cast(img, dtype=tf.int32)\n",
    "\n",
    "    # Implement model\n",
    "    outputs = movenet(img)\n",
    "    outputs = outputs['output_0'].numpy()[:,:,:51].reshape((6,17,3)) # Get keypoints with scores\n",
    "    y, x, c = frame.shape\n",
    "    outputs = np.squeeze(np.multiply(outputs, [y,x,1])) # Multiply normalized coordinates by image size\n",
    "    return outputs\n",
    "\n",
    "# Get live video\n",
    "def get_video(cap, edges, confidence_interval, flag):\n",
    "    assert flag == 0 or flag == 1\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    outputs = get_outputs(frame)\n",
    "\n",
    "    loop_through_people(frame, outputs, edges, confidence_interval)\n",
    "    if flag == 0:\n",
    "        cv2.imshow('Movenet Multipose 1', frame) # Show live frame\n",
    "    elif flag == 1:\n",
    "        cv2.imshow('Movenet Multipose 2', frame) # Show live frame\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Get output boolean matrix to only include points with high confidence in both images\n",
    "def get_output_boolean(outputs1, outputs2, confidence_interval):\n",
    "    outputs_bool1 = outputs1[:,:,2]>confidence_interval\n",
    "    outputs_bool2 = outputs2[:,:,2]>confidence_interval\n",
    "    outputs_bool = outputs_bool1 & outputs_bool2\n",
    "    return outputs_bool\n",
    "\n",
    "# Get the undistorted points from the outputs of the Movenet Model\n",
    "def get_undistorted(outputs, calib):\n",
    "    undistorted = np.full((17, 2), False)\n",
    "    for i in np.arange(np.size(outputs[:,0,0])):\n",
    "        pts = outputs[i,:,:2]\n",
    "        pts = np.squeeze(cv2.undistortPoints(np.float32(pts), calib['mtx'], calib['dist']))\n",
    "        pts[:,0] = pts[:,0]*calib['mtx'][0,0]+calib['mtx'][0,2]\n",
    "        pts[:,1] = pts[:,1]*calib['mtx'][1,1]+calib['mtx'][1,2]\n",
    "        undistorted = np.dstack((undistorted, pts))\n",
    "    undistorted = np.swapaxes(np.swapaxes(undistorted[:, :, 1:], 0, 2), 1, 2)\n",
    "    return undistorted\n",
    "\n",
    "# Calculate depth using triangulation between the two images\n",
    "# Source: https://link.springer.com/article/10.1007/s11263-017-1036-4\n",
    "def get_depth(undistorted1, undistorted2, baseline, calib):\n",
    "    depth = np.full((17), False)\n",
    "    for i in np.arange(np.size(undistorted1[:,0,0])):\n",
    "        depth_i = calib['mtx'][0,0]*baseline/abs(undistorted1[i,:,1]-undistorted2[i,:,1])\n",
    "        depth = np.dstack((depth, depth_i))\n",
    "    depth = np.swapaxes(np.squeeze(depth[:,:,1:]), 0, 1)\n",
    "    return depth\n",
    "\n",
    "# Return 3D coordinates based on World Coordinate unit vector multiplied by depth\n",
    "def get_3d_coord(undistorted, depth, calib, outputs_bool):\n",
    "    undistorted = np.dstack((undistorted, np.ones((6,17))))\n",
    "    undistorted = np.swapaxes(undistorted, 1, 2)\n",
    "    temp=undistorted.copy()\n",
    "    undistorted[:,0,:]=temp[:,1,:]; undistorted[:,1,:]=temp[:,0,:]; del temp # Rewrite undistorted for [x,y] rather than [y,x]\n",
    "    world = np.swapaxes(np.dot(np.linalg.inv(calib['mtx']), undistorted), 0, 1)\n",
    "    depth = np.swapaxes(np.dstack((depth, depth, depth)), 1, 2) # To multiply the [x,y,z] coordinates\n",
    "    coords = depth*world # Multiply by depth\n",
    "    outputs_bool = np.swapaxes(np.dstack((outputs_bool,outputs_bool,outputs_bool)), 1, 2)\n",
    "    coords = coords*outputs_bool # Only use points in both images\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "confidence_interval = 0.4 # Adjust depending on accuracy requirements\n",
    "baseline = 6 # note your unit (inches)\n",
    "calib1 = np.load('internal_webcam_calibration.npz') # Load in calib 1\n",
    "calib2 = np.load('external_webcam_calibration.npz') # Load in calib 2\n",
    "outputs_bool = np.full((6,17), False) # Instantiate outputs_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam \n",
    "cap1 = cv2.VideoCapture(1) # (may have to adjust x in cv2.VideoCapture(x) to proper webcam (0, 1, 2... etc.))\n",
    "cap2 = cv2.VideoCapture(0) # Note which webcam is which\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "fig = plt.figure()\n",
    "plt.style.use('fivethirtyeight')\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "while cap1.isOpened() or cap2.isOpened():\n",
    "    outputs1 = get_video(cap1, edges, confidence_interval, 0)\n",
    "    outputs2 = get_video(cap2, edges, confidence_interval, 1)\n",
    "    \n",
    "    # Return 3D coordinates in each camera coordinates\n",
    "    outputs_bool = get_output_boolean(outputs1, outputs2, confidence_interval)\n",
    "    undistorted1 = get_undistorted(outputs1, calib1)\n",
    "    undistorted2 = get_undistorted(outputs2, calib2)\n",
    "    depth1 = get_depth(undistorted1, undistorted2, baseline, calib1)\n",
    "    depth2 = get_depth(undistorted1, undistorted2, baseline, calib2)\n",
    "    coords1 = get_3d_coord(undistorted1, depth1, calib1, outputs_bool)\n",
    "    coords2 = get_3d_coord(undistorted2, depth2, calib2, outputs_bool)\n",
    "\n",
    "    # Plot 3D coordinates using Matplotlib\n",
    "    ax.cla()\n",
    "    ax.plot(coords1[0,0,:], coords1[0,2,:], -coords1[0,1,:], 'xb') # reorient for correct visualization\n",
    "    for edge, color in edges.items(): # Loop through every coordinate and connect points like in connect_keypoints\n",
    "        p1, p2 = edge\n",
    "        x1, y1, z1 = coords1[0,:,p1]\n",
    "        x2, y2, z2 = coords1[0,:,p2]\n",
    "        if x1 != 0 and x2 !=0:\n",
    "            ax.plot([x1, x2], [z1, z2], [-y1, -y2], '-r')\n",
    "    ax.set_xlabel('X'); ax.set_ylabel('Z'), ax.set_zlabel('Y')\n",
    "    ax.set_xlim([-30, 30]); ax.set_ylim([0, 50]); ax.set_zlim([-30, 30]) # Set axes of graph\n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.0001)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'): # Press q to quit live stream\n",
    "        break\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13fa9698f0cf9a95b1e3ac14834208a2a3f67f6514b973e68ad06ec34ce12b5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
